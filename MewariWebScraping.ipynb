{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddee8c76",
   "metadata": {},
   "source": [
    "#                                                ASSIGNMENT-1\n",
    "#                                                WEB SCRAPING\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2962737",
   "metadata": {},
   "source": [
    "## In all the following questions, you have to use BeautifulSoup to scrape different websites and collect data as per\n",
    "## the requirement of the question.\n",
    "## Every answer to the question should be in form of a python function which should take URL as the parameter.\n",
    "## Use Jupyter Notebooks to program, upload it on your GitHub and send the link of the Jupyter notebook to your\n",
    "## SME."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231b0ce4",
   "metadata": {},
   "source": [
    "### Q1 Write a python program to display all the header tags from wikipedia.org and make data frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8054d19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Header_Tags\n",
      "0                      Main Page\n",
      "1           Welcome to Wikipedia\n",
      "2  From today's featured article\n",
      "3               Did you know ...\n",
      "4                    In the news\n",
      "5                    On this day\n",
      "6       Today's featured picture\n",
      "7       Other areas of Wikipedia\n",
      "8    Wikipedia's sister projects\n",
      "9            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "#importing the required libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def wikheaders(url):\n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    headers=[]\n",
    "    for i in soup.find_all(['h1', 'h2','h3','h4','h5','h6']):\n",
    "        headers.append(i.text)\n",
    "    \n",
    "    df = pd.DataFrame({'Header_Tags':headers})\n",
    "    print(df)\n",
    "    \n",
    "    \n",
    "wikheaders('https://en.wikipedia.org/wiki/Main_Page')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83043c51",
   "metadata": {},
   "source": [
    "### Q2 Write s python program to display list of respected former presidents of India(i.e. Name , Term of office)\n",
    "### from https://presidentofindia.nic.in/former-presidents.htm and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3d67732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Former_President Name  \\\n",
      "0           Shri Ram Nath Kovind    \n",
      "1          Shri Pranab Mukherjee    \n",
      "2   Smt Pratibha Devisingh Patil    \n",
      "3         DR. A.P.J. Abdul Kalam    \n",
      "4           Shri K. R. Narayanan    \n",
      "5        Dr Shankar Dayal Sharma    \n",
      "6            Shri R Venkataraman    \n",
      "7               Giani Zail Singh    \n",
      "8      Shri Neelam Sanjiva Reddy    \n",
      "9       Dr. Fakhruddin Ali Ahmed    \n",
      "10  Shri Varahagiri Venkata Giri    \n",
      "11              Dr. Zakir Husain    \n",
      "12  Dr. Sarvepalli Radhakrishnan    \n",
      "13           Dr. Rajendra Prasad    \n",
      "\n",
      "                                       Term of Office  \n",
      "0                     25 July, 2017 to 25 July, 2022   \n",
      "1                     25 July, 2012 to 25 July, 2017   \n",
      "2                     25 July, 2007 to 25 July, 2012   \n",
      "3                     25 July, 2002 to 25 July, 2007   \n",
      "4                     25 July, 1997 to 25 July, 2002   \n",
      "5                     25 July, 1992 to 25 July, 1997   \n",
      "6                     25 July, 1987 to 25 July, 1992   \n",
      "7                     25 July, 1982 to 25 July, 1987   \n",
      "8                     25 July, 1977 to 25 July, 1982   \n",
      "9                24 August, 1974 to 11 February, 1977  \n",
      "10   3 May, 1969 to 20 July, 1969 and 24 August, 1...  \n",
      "11                        13 May, 1967 to 3 May, 1969  \n",
      "12                       13 May, 1962 to 13 May, 1967  \n",
      "13                   26 January, 1950 to 13 May, 1962  \n"
     ]
    }
   ],
   "source": [
    "#importing the required libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def president(url):\n",
    "    page = requests.get(url)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    #FormerPresident_Names\n",
    "    select_class =\"presidentListing\"\n",
    "    president_tags=soup.find_all('div',{'class':select_class})\n",
    "    Names=[]\n",
    "    \n",
    "    for tag in president_tags:\n",
    "        title = tag.find('h3').text.split('(')[0]\n",
    "        Names.append(title)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #FormerPresident_TermOfOffice\n",
    "    \n",
    "    year_class =\"presidentListing\"\n",
    "    term_tags=soup.find_all('div',{'class':year_class})\n",
    "    Office=[]\n",
    "    \n",
    "    for tag in term_tags:\n",
    "        term_year = tag.find('p').text.split(':')[1]\n",
    "        Office.append(term_year)\n",
    "    \n",
    "    Former_President=pd.DataFrame({'Former_President Name':Names,'Term of Office':Office})\n",
    "    \n",
    "    print(Former_President)\n",
    "    \n",
    "    \n",
    "president('https://presidentofindia.nic.in/former-presidents.htm')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aea23b7",
   "metadata": {},
   "source": [
    "### Q3 Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72df8f6",
   "metadata": {},
   "source": [
    "### a)Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db960fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Top 10 team names Matches Rating\n",
      "0         Australia      35    113\n",
      "1       New Zealand      31    113\n",
      "2             India      47    113\n",
      "3           England      36    111\n",
      "4          Pakistan      25    106\n",
      "5      South Africa      31    101\n",
      "6        Bangladesh      38     95\n",
      "7         Sri Lanka      36     86\n",
      "8       West Indies      43     72\n",
      "9       Afghanistan      20     71\n"
     ]
    }
   ],
   "source": [
    "#importig required libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def team(url):\n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    #top 10 ODI team names\n",
    "    team=[] #empty list for storing yhe titles\n",
    "    for i in soup.find_all('span', class_=\"u-hide-phablet\")[:10]:\n",
    "        team.append(i.text)\n",
    "    \n",
    "    #top 10 ODI teams matches\n",
    "    matches=[]\n",
    "        #for 1st team\n",
    "    for i in soup.find('td',class_='rankings-block__banner--matches'):\n",
    "        matches.append(i.text)\n",
    "        #for other teams\n",
    "    for i in soup.find_all('td',class_='table-body__cell u-center-text')[0:18:2]:\n",
    "        matches.append(i.text)\n",
    "    \n",
    "    #top teams rating\n",
    "    rating =[]\n",
    "      #for 1st team\n",
    "    for i in soup.find_all('td',class_='rankings-block__banner--rating u-text-right'):\n",
    "        rating.append(i.get_text().strip())\n",
    "        #for other teams\n",
    "    for i in soup.find_all('td',class_='table-body__cell u-text-right rating')[:9]:\n",
    "        rating.append(i.text)\n",
    "    \n",
    "   \n",
    "   #Top 10 team names':team,'Matches':matches\n",
    "    k=pd.DataFrame({'Top 10 team names':team,'Matches':matches,'Rating':rating})\n",
    "    print(k)\n",
    "         \n",
    "team('https://www.icc-cricket.com/rankings/mens/team-rankings/odi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575b8e72",
   "metadata": {},
   "source": [
    "### b) Top 10 ODI Batsmen along with the records of their team and rating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "94945949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Top 10 Batsman Name Country Rating\n",
      "0             Babar Azam     PAK    887\n",
      "1  Rassie van der Dussen      SA    777\n",
      "2            Imam-ul-Haq     PAK    740\n",
      "3           Shubman Gill     IND    738\n",
      "4           David Warner     AUS    726\n",
      "5            Virat Kohli     IND    719\n",
      "6        Quinton de Kock      SA    718\n",
      "7           Rohit Sharma     IND    707\n",
      "8            Steve Smith     AUS    702\n",
      "9           Fakhar Zaman     PAK    699\n"
     ]
    }
   ],
   "source": [
    "#importig required libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def Batsmen(url):\n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    #top 10 Batsman names\n",
    "    man=[] #empty list for storing yhe titles\n",
    "    #for no.1 batsmen\n",
    "    for i in soup.find_all('div', class_=\"rankings-block__banner--name\")[:1]:\n",
    "        man.append(i.text)\n",
    "    #for rest batsman\n",
    "    findman_tags=soup.find_all('td', class_=\"table-body__cell name\")[:9]\n",
    "    for tag in findman_tags:\n",
    "        mann=tag.find('a').text\n",
    "        man.append(mann)\n",
    "        \n",
    "        \n",
    "    #top 10 ODI Batsman team names\n",
    "    country=[]\n",
    "    #for no.1 batsmen\n",
    "    for i in soup.find_all('div', class_=\"rankings-block__banner--nationality\")[:1]:\n",
    "        country.append(i.text.split()[0])\n",
    "    #for rest batsman\n",
    "    for i in soup.find_all('span',class_='table-body__logo-text')[:9]:\n",
    "        country.append(i.text)\n",
    "   \n",
    "\n",
    "    #top 10 batsman rating\n",
    "     #for no.1 batsmen\n",
    "    rating=[]\n",
    "    for i in soup.find_all('div', class_=\"rankings-block__banner--rating\")[:1]:\n",
    "        rating.append(i.text.split()[0])\n",
    "    #for rest batsman\n",
    "    for i in soup.find_all('td',class_='table-body__cell u-text-right rating')[:9]:\n",
    "        rating.append(i.text)\n",
    "    \n",
    "   \n",
    "   \n",
    "    k=pd.DataFrame({'Top 10 Batsman Name':man,'Country':country,'Rating':rating})\n",
    "    print(k)\n",
    "         \n",
    "Batsmen('https://www.icc-cricket.com/rankings/mens/player-rankings/odi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b786d8e2",
   "metadata": {},
   "source": [
    "### c) Top 10 ODI bowlers along with the records of their team andrating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62371325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Top 10 ODI Bowler's Name Country Rating\n",
      "0           Josh Hazlewood     AUS    705\n",
      "1              Trent Boult      NZ    694\n",
      "2           Mohammed Siraj     IND    691\n",
      "3           Mitchell Starc     AUS    686\n",
      "4               Matt Henry      NZ    676\n",
      "5              Rashid Khan     AFG    659\n",
      "6               Adam Zampa     AUS    652\n",
      "7           Shaheen Afridi     PAK    641\n",
      "8         Mujeeb Ur Rahman     AFG    637\n",
      "9          Shakib Al Hasan     BAN    636\n"
     ]
    }
   ],
   "source": [
    "#importig required libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def Bowler(url):\n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    #top 10 Bowlers names\n",
    "    man=[] #empty list for storing yhe titles\n",
    "    #for no.1 bowler\n",
    "    for i in soup.find_all('div', class_=\"rankings-block__banner--name\")[1:2]:\n",
    "        man.append(i.text)\n",
    "    #for rest bowler's\n",
    "    findman_tags=soup.find_all('td', class_=\"table-body__cell name\")[9:18]\n",
    "    for tag in findman_tags:\n",
    "        mann=tag.find('a').text\n",
    "        man.append(mann)\n",
    "        \n",
    "        \n",
    "    #top 10 ODI Bowler's team names\n",
    "    country=[]\n",
    "    #for no.1 bowler\n",
    "    for i in soup.find_all('div', class_=\"rankings-block__banner--nationality\")[1:2]:\n",
    "        country.append(i.text.split()[0])\n",
    "    #for rest bowler's\n",
    "    for i in soup.find_all('span',class_='table-body__logo-text')[9:18]:\n",
    "        country.append(i.text)\n",
    "   \n",
    "\n",
    "    #top 10 bowler's rating\n",
    "     #for no.1 bowler's\n",
    "    rating=[]\n",
    "    for i in soup.find_all('div', class_=\"rankings-block__banner--rating\")[1:2]:\n",
    "        rating.append(i.text.split()[0])\n",
    "    #for rest bowler's\n",
    "    for i in soup.find_all('td',class_='table-body__cell u-text-right rating')[9:18]:\n",
    "        rating.append(i.text)\n",
    "    \n",
    "   \n",
    "   \n",
    "    k=pd.DataFrame({\"Top 10 ODI Bowler's Name\":man,'Country':country,'Rating':rating})\n",
    "    print(k)\n",
    "         \n",
    "Bowler('https://www.icc-cricket.com/rankings/mens/player-rankings/odi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ba05e9",
   "metadata": {},
   "source": [
    "### Q4 Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227701ea",
   "metadata": {},
   "source": [
    "### a)Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3c79456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Top 10 team names Matches Rating\n",
      "0         Australia      21    172\n",
      "1           England      28    119\n",
      "2      South Africa      26    119\n",
      "3             India      27    104\n",
      "4       New Zealand      25    102\n",
      "5       West Indies      27     94\n",
      "6        Bangladesh      13     76\n",
      "7          Thailand      11     75\n",
      "8          Pakistan      27     62\n",
      "9         Sri Lanka       8     44\n"
     ]
    }
   ],
   "source": [
    "#importig required libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def Batsmen_name(url):\n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    #top 10 ODI team names\n",
    "    team=[] #empty list for storing yhe titles\n",
    "    for i in soup.find_all('span', class_=\"u-hide-phablet\")[:10]:\n",
    "        team.append(i.text)\n",
    "    \n",
    "    #top 10 ODI teams matches\n",
    "    matches=[]\n",
    "        #for 1st team\n",
    "    for i in soup.find('td',class_='rankings-block__banner--matches'):\n",
    "        matches.append(i.text)\n",
    "        #for other teams\n",
    "    for i in soup.find_all('td',class_='table-body__cell u-center-text')[0:18:2]:\n",
    "        matches.append(i.text)\n",
    "    \n",
    "    #top teams rating\n",
    "    rating =[]\n",
    "      #for 1st team\n",
    "    for i in soup.find_all('td',class_='rankings-block__banner--rating u-text-right'):\n",
    "        rating.append(i.get_text().strip())\n",
    "        #for other teams\n",
    "    for i in soup.find_all('td',class_='table-body__cell u-text-right rating')[:9]:\n",
    "        rating.append(i.text)\n",
    "    \n",
    "   \n",
    "   #Top 10 team names':team,'Matches':matches\n",
    "    k=pd.DataFrame({'Top 10 team names':team,'Matches':matches,'Rating':rating})\n",
    "    print(k)\n",
    "         \n",
    "Batsmen_name('https://www.icc-cricket.com/rankings/womens/team-rankings/odi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4557748",
   "metadata": {},
   "source": [
    "### b) Top 10 women’s ODI Batting players along with the records of their team and rating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60d34eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Top 10 Batsman Name Country Rating\n",
      "0         Alyssa Healy     AUS    762\n",
      "1          Beth Mooney     AUS    754\n",
      "2      Laura Wolvaardt      SA    732\n",
      "3       Natalie Sciver     ENG    731\n",
      "4          Meg Lanning     AUS    717\n",
      "5     Harmanpreet Kaur     IND    716\n",
      "6      Smriti Mandhana     IND    714\n",
      "7  Chamari Athapaththu      SL    655\n",
      "8    Amy Satterthwaite      NZ    641\n",
      "9         Ellyse Perry     AUS    626\n"
     ]
    }
   ],
   "source": [
    "#importig required libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def Batsmen(url):\n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    #top 10 Batsman names\n",
    "    man=[] #empty list for storing yhe titles\n",
    "    #for no.1 batsmen\n",
    "    for i in soup.find_all('div', class_=\"rankings-block__banner--name\")[:1]:\n",
    "        man.append(i.text)\n",
    "    #for rest batsman\n",
    "    findman_tags=soup.find_all('td', class_=\"table-body__cell name\")[:9]\n",
    "    for tag in findman_tags:\n",
    "        mann=tag.find('a').text\n",
    "        man.append(mann)\n",
    "        \n",
    "        \n",
    "    #top 10 ODI Batsman team names\n",
    "    country=[]\n",
    "    #for no.1 batsmen\n",
    "    for i in soup.find_all('div', class_=\"rankings-block__banner--nationality\")[:1]:\n",
    "        country.append(i.text.split()[0])\n",
    "    #for rest batsman\n",
    "    for i in soup.find_all('span',class_='table-body__logo-text')[:9]:\n",
    "        country.append(i.text)\n",
    "   \n",
    "\n",
    "    #top 10 batsman rating\n",
    "     #for no.1 batsmen\n",
    "    rating=[]\n",
    "    for i in soup.find_all('div', class_=\"rankings-block__banner--rating\")[:1]:\n",
    "        rating.append(i.text.split()[0])\n",
    "    #for rest batsman\n",
    "    for i in soup.find_all('td',class_='table-body__cell u-text-right rating')[:9]:\n",
    "        rating.append(i.text)\n",
    "    \n",
    "   \n",
    "   \n",
    "    k=pd.DataFrame({'Top 10 Women Batting Name':man,'Country':country,'Rating':rating})\n",
    "    print(k)\n",
    "         \n",
    "Batsmen('https://www.icc-cricket.com/rankings/womens/player-rankings/odi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a97507",
   "metadata": {},
   "source": [
    "### c) Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6ec00b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Top 10 ODI Bowler's Name Country Rating\n",
      "0          Hayley Matthews      WI    373\n",
      "1           Natalie Sciver     ENG    371\n",
      "2             Ellyse Perry     AUS    366\n",
      "3           Marizanne Kapp      SA    349\n",
      "4              Amelia Kerr      NZ    336\n",
      "5            Deepti Sharma     IND    322\n",
      "6         Ashleigh Gardner     AUS    292\n",
      "7            Jess Jonassen     AUS    250\n",
      "8                 Nida Dar     PAK    232\n",
      "9        Sophie Ecclestone     ENG    205\n"
     ]
    }
   ],
   "source": [
    "#importig required libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def all_rounder(url):\n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    #top 10 Bowlers names\n",
    "    man=[] #empty list for storing yhe titles\n",
    "    #for no.1 bowler\n",
    "    for i in soup.find_all('div', class_=\"rankings-block__banner--name\")[2:3]:\n",
    "        man.append(i.text)\n",
    "    #for rest bowler's\n",
    "    findman_tags=soup.find_all('td', class_=\"table-body__cell name\")[18:28]\n",
    "    for tag in findman_tags:\n",
    "        mann=tag.find('a').text\n",
    "        man.append(mann)\n",
    "        \n",
    "        \n",
    "    #top 10 ODI Bowler's team names\n",
    "    country=[]\n",
    "    #for no.1 bowler\n",
    "    for i in soup.find_all('div', class_=\"rankings-block__banner--nationality\")[2:3]:\n",
    "        country.append(i.text.split()[0])\n",
    "    #for rest bowler's\n",
    "    for i in soup.find_all('span',class_='table-body__logo-text')[18:28]:\n",
    "        country.append(i.text)\n",
    "   \n",
    "\n",
    "    #top 10 bowler's rating\n",
    "     #for no.1 bowler's\n",
    "    rating=[]\n",
    "    for i in soup.find_all('div', class_=\"rankings-block__banner--rating\")[2:3]:\n",
    "        rating.append(i.text.split()[0])\n",
    "    #for rest bowler's\n",
    "    for i in soup.find_all('td',class_='table-body__cell u-text-right rating')[18:28]:\n",
    "        rating.append(i.text)\n",
    "    \n",
    "   \n",
    "   \n",
    "    k=pd.DataFrame({\"Top 10 ODI Bowler's Name\":man,'Country':country,'Rating':rating})\n",
    "    print(k)\n",
    "         \n",
    "all_rounder('https://www.icc-cricket.com/rankings/womens/player-rankings/odi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6979aecc",
   "metadata": {},
   "source": [
    "### Q5) Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world and make data frame\n",
    "### i)Headline\n",
    "### ii) Time\n",
    "### iii) News Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1b283acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Headline            Time  \\\n",
      "0   What Home Depot's billion-dollar pay raise may...     2 Hours Ago   \n",
      "1   Sales, spin-offs and splits — the differences ...     2 Hours Ago   \n",
      "2   Charlie Munger reportedly warns of trouble for...     2 Hours Ago   \n",
      "3   Workers are secretly using ChatGPT, AI and it ...     2 Hours Ago   \n",
      "4   A psychologist shares 6 toxic phrases 'highly ...     3 Hours Ago   \n",
      "5   Top cloud providers Amazon, Microsoft and Goog...     4 Hours Ago   \n",
      "6   Alzheimer’s patients may wait years for new tr...     4 Hours Ago   \n",
      "7       Retirees are flocking to these 10 U.S. states     4 Hours Ago   \n",
      "8   It might be Jonah Peretti's last chance to tur...     5 Hours Ago   \n",
      "9   Obesity is a chronic disease. It's why treatin...     5 Hours Ago   \n",
      "10  Mario Gabelli ranks old media stocks among his...     5 Hours Ago   \n",
      "11  Earnings playbook: The second half of the seas...     5 Hours Ago   \n",
      "12  SpaceX to spend $2 billion on Starship this ye...    15 Hours Ago   \n",
      "13  Jack Dorsey criticizes Elon Musk’s leadership ...    19 Hours Ago   \n",
      "14  Banks including JPMorgan Chase, Bank of Americ...    23 Hours Ago   \n",
      "15  How to raise a kids with a ‘secure’ attachment...  April 29, 2023   \n",
      "16  The business case for green sports stadiums an...  April 29, 2023   \n",
      "17  A solid start to earnings season will be put t...  April 29, 2023   \n",
      "18  TikTokers are using beef tallow to treat acne:...  April 29, 2023   \n",
      "19  This 28-year-old pays $62 a month to live in a...  April 29, 2023   \n",
      "20  As part of the  ‘cocktail culture,' consumers ...  April 29, 2023   \n",
      "21  Why GM is killing the Chevy Bolt — America's c...  April 29, 2023   \n",
      "22  Just a few stocks are behind the market's resi...  April 29, 2023   \n",
      "23  Google Cloud's Kurian on road to profit: 'We w...  April 29, 2023   \n",
      "24  Olipop nears $200 million in annual sales—and ...  April 29, 2023   \n",
      "25  This 25-year-old makes $200/hour without a bac...  April 29, 2023   \n",
      "26  The most overbought and oversold S&P 500 stock...  April 29, 2023   \n",
      "27  Mark Cuban says paying Twitter for a blue chec...  April 29, 2023   \n",
      "28  Goldman Sachs says these are its top picks com...  April 29, 2023   \n",
      "29  What’s next for SpaceX’s Starship after a dram...  April 29, 2023   \n",
      "\n",
      "                                            News Link  \n",
      "0   https://www.cnbc.com/2023/04/30/what-home-depo...  \n",
      "1   https://www.cnbc.com/2023/04/30/sales-spin-off...  \n",
      "2   https://www.cnbc.com/2023/04/30/charlie-munger...  \n",
      "3   https://www.cnbc.com/2023/04/30/the-big-cyber-...  \n",
      "4   https://www.cnbc.com/2023/04/30/psychologist-s...  \n",
      "5   https://www.cnbc.com/2023/04/30/cloud-provider...  \n",
      "6   https://www.cnbc.com/2023/04/30/alzheimers-tre...  \n",
      "7   https://www.cnbc.com/2023/04/30/florida-is-the...  \n",
      "8   https://www.cnbc.com/2023/04/30/buzzfeed-turna...  \n",
      "9   https://www.cnbc.com/2023/04/30/obesity-is-a-c...  \n",
      "10  https://www.cnbc.com/2023/04/30/mario-gabelli-...  \n",
      "11  https://www.cnbc.com/2023/04/30/earnings-playb...  \n",
      "12  https://www.cnbc.com/2023/04/29/elon-musk-spac...  \n",
      "13  https://www.cnbc.com/2023/04/29/jack-dorsey-cr...  \n",
      "14  https://www.cnbc.com/2023/04/29/first-republic...  \n",
      "15  https://www.cnbc.com/2023/04/29/how-to-raise-a...  \n",
      "16  https://www.cnbc.com/2023/04/29/the-business-c...  \n",
      "17  https://www.cnbc.com/2023/04/29/decent-start-t...  \n",
      "18  https://www.cnbc.com/2023/04/29/tiktokers-are-...  \n",
      "19  https://www.cnbc.com/2023/04/29/28-year-old-pa...  \n",
      "20  https://www.cnbc.com/2023/04/29/cocktail-cultu...  \n",
      "21  https://www.cnbc.com/2023/04/29/why-gm-is-kill...  \n",
      "22  https://www.cnbc.com/2023/04/29/just-a-few-sto...  \n",
      "23  https://www.cnbc.com/2023/04/29/google-cloud-b...  \n",
      "24  https://www.cnbc.com/2023/04/29/olipop-nears-2...  \n",
      "25  https://www.cnbc.com/2023/04/29/25-year-old-no...  \n",
      "26  https://www.cnbc.com/2023/04/29/herea-re-the-m...  \n",
      "27  https://www.cnbc.com/2023/04/29/mark-cuban-pay...  \n",
      "28  https://www.cnbc.com/2023/04/29/goldman-sachs-...  \n",
      "29  https://www.cnbc.com/2023/04/29/spacex-starshi...  \n"
     ]
    }
   ],
   "source": [
    "#importing the required libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def news(url):\n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    #For headlines\n",
    "    headers=[]\n",
    "    for i in soup.find_all('a',class_='LatestNews-headline'):\n",
    "        headers.append(i.text)\n",
    "    \n",
    "    #for time\n",
    "    time=[]\n",
    " \n",
    "    for i in soup.find_all('time',class_='LatestNews-timestamp'):\n",
    "        time.append(i.text)\n",
    "    #for url\n",
    "    url=[]\n",
    "    for i in soup.find_all('a',class_='LatestNews-headline'):\n",
    "        url.append(i.get('href'))\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "    df=pd.DataFrame({'Headline':headers,'Time':time,'News Link':url})\n",
    "    print(df)\n",
    "   \n",
    "news('https://www.cnbc.com/world/?region=world')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b502b3",
   "metadata": {},
   "source": [
    "### Q6Write a python program to scrape the details of most downloaded articles from AI in last 90\n",
    "### days.https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
    "### Scrape below mentioned details and make data frame\n",
    "### i)Paper Title\n",
    "### ii) Authors\n",
    "### iii) Published Date\n",
    "### iv) Paper URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5e2acf33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Paper Title  \\\n",
      "0                                    Reward is enough   \n",
      "1                           Making sense of raw input   \n",
      "2   Law and logic: A review from an argumentation ...   \n",
      "3              Creativity and artificial intelligence   \n",
      "4   Artificial cognition for social human–robot in...   \n",
      "5   Explanation in artificial intelligence: Insigh...   \n",
      "6                       Making sense of sensory input   \n",
      "7   Conflict-based search for optimal multi-agent ...   \n",
      "8   Between MDPs and semi-MDPs: A framework for te...   \n",
      "9   The Hanabi challenge: A new frontier for AI re...   \n",
      "10  Evaluating XAI: A comparison of rule-based and...   \n",
      "11           Argumentation in artificial intelligence   \n",
      "12  Algorithms for computing strategies in two-pla...   \n",
      "13      Multiple object tracking: A literature review   \n",
      "14  Selection of relevant features and examples in...   \n",
      "15  A survey of inverse reinforcement learning: Ch...   \n",
      "16  Explaining individual predictions when feature...   \n",
      "17  A review of possible effects of cognitive bias...   \n",
      "18  Integrating social power into the decision-mak...   \n",
      "19  “That's (not) the output I expected!” On the r...   \n",
      "20  Explaining black-box classifiers using post-ho...   \n",
      "21  Algorithm runtime prediction: Methods & evalua...   \n",
      "22              Wrappers for feature subset selection   \n",
      "23  Commonsense visual sensemaking for autonomous ...   \n",
      "24         Quantum computation, quantum theory and AI   \n",
      "\n",
      "                                              Authors  Published Date  \\\n",
      "0   Silver, David, Singh, Satinder, Precup, Doina,...    October 2021   \n",
      "1           Evans, Richard, Bošnjak, Matko and 5 more    October 2021   \n",
      "2                   Prakken, Henry, Sartor, Giovanni     October 2015   \n",
      "3                                 Boden, Margaret A.      August 1998   \n",
      "4     Lemaignan, Séverin, Warnier, Mathieu and 3 more       June 2017   \n",
      "5                                        Miller, Tim    February 2019   \n",
      "6   Evans, Richard, Hernández-Orallo, José and 3 more      April 2021   \n",
      "7   Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...   February 2015   \n",
      "8   Sutton, Richard S., Precup, Doina, Singh, Sati...     August 1999   \n",
      "9         Bard, Nolan, Foerster, Jakob N. and 13 more      March 2020   \n",
      "10  van der Waa, Jasper, Nieuwburg, Elisabeth, Cre...   February 2021   \n",
      "11               Bench-Capon, T.J.M., Dunne, Paul E.     October 2007   \n",
      "12       Bošanský, Branislav, Lisý, Viliam and 3 more     August 2016   \n",
      "13             Luo, Wenhan, Xing, Junliang and 4 more      April 2021   \n",
      "14                      Blum, Avrim L., Langley, Pat    December 1997   \n",
      "15                   Arora, Saurabh, Doshi, Prashant      August 2021   \n",
      "16      Aas, Kjersti, Jullum, Martin, Løland, Anders   September 2021   \n",
      "17  Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Joha...       June 2021   \n",
      "18    Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.    December 2016   \n",
      "19                      Riveiro, Maria, Thill, Serge   September 2021   \n",
      "20  Kenny, Eoin M., Ford, Courtney, Quinn, Molly, ...        May 2021   \n",
      "21  Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyto...    January 2014   \n",
      "22                      Kohavi, Ron, John, George H.    December 1997   \n",
      "23  Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srik...    October 2021   \n",
      "24                                   Ying, Mingsheng    February 2010   \n",
      "\n",
      "                                            Paper URL  \n",
      "0   https://www.sciencedirect.com/science/article/...  \n",
      "1   https://www.sciencedirect.com/science/article/...  \n",
      "2   https://www.sciencedirect.com/science/article/...  \n",
      "3   https://www.sciencedirect.com/science/article/...  \n",
      "4   https://www.sciencedirect.com/science/article/...  \n",
      "5   https://www.sciencedirect.com/science/article/...  \n",
      "6   https://www.sciencedirect.com/science/article/...  \n",
      "7   https://www.sciencedirect.com/science/article/...  \n",
      "8   https://www.sciencedirect.com/science/article/...  \n",
      "9   https://www.sciencedirect.com/science/article/...  \n",
      "10  https://www.sciencedirect.com/science/article/...  \n",
      "11  https://www.sciencedirect.com/science/article/...  \n",
      "12  https://www.sciencedirect.com/science/article/...  \n",
      "13  https://www.sciencedirect.com/science/article/...  \n",
      "14  https://www.sciencedirect.com/science/article/...  \n",
      "15  https://www.sciencedirect.com/science/article/...  \n",
      "16  https://www.sciencedirect.com/science/article/...  \n",
      "17  https://www.sciencedirect.com/science/article/...  \n",
      "18  https://www.sciencedirect.com/science/article/...  \n",
      "19  https://www.sciencedirect.com/science/article/...  \n",
      "20  https://www.sciencedirect.com/science/article/...  \n",
      "21  https://www.sciencedirect.com/science/article/...  \n",
      "22  https://www.sciencedirect.com/science/article/...  \n",
      "23  https://www.sciencedirect.com/science/article/...  \n",
      "24  https://www.sciencedirect.com/science/article/...  \n"
     ]
    }
   ],
   "source": [
    "#importing the required libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def AI(url):\n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    #For Paper titles\n",
    "    titles=[]\n",
    "    for i in soup.find_all('h2',class_='sc-1qrq3sd-1 gRGSUS sc-1nmom32-0 sc-1nmom32-1 btcbYu goSKRg'):\n",
    "        titles.append(i.text)\n",
    "    #For Authors\n",
    "    authors=[]\n",
    "    for i in soup.find_all('span',class_='sc-1w3fpd7-0 dnCnAO'):\n",
    "        authors.append(i.text)\n",
    "   \n",
    "    #For Published Date\n",
    "    date=[]\n",
    "    for i in soup.find_all('span',class_='sc-1thf9ly-2 dvggWt'):\n",
    "        date.append(i.text)\n",
    "    \n",
    "    #For URL\n",
    "    url=[]\n",
    "    for i in soup.find_all('a',class_='sc-5smygv-0 fIXTHm'):\n",
    "        url.append(i.get('href'))\n",
    "    \n",
    "    df=pd.DataFrame({'Paper Title':titles,'Authors':authors,'Published Date':date,'Paper URL':url})\n",
    "    print(df)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "AI('https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c306172",
   "metadata": {},
   "source": [
    "### 7) Write a python program to scrape mentioned details from dineout.co.inand make data frame\n",
    "### i) Restaurant name\n",
    "### ii) Cuisine\n",
    "### iii) Location\n",
    "### iv) Ratings\n",
    "### v) Image URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f7ff823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Restaurant Name  \\\n",
      "0                   Castle Barbeque   \n",
      "1                        Cafe Knosh   \n",
      "2                       India Grill   \n",
      "3              The Barbeque Company   \n",
      "4                    Delhi Barbeque   \n",
      "5  The Monarch - Bar Be Que Village   \n",
      "6                 Indian Grill Room   \n",
      "7                The Barbeque Times   \n",
      "\n",
      "                                             Cuisine  \\\n",
      "0                              Chinese, North Indian   \n",
      "1                               Italian, Continental   \n",
      "2                              North Indian, Italian   \n",
      "3                              North Indian, Chinese   \n",
      "4                                       North Indian   \n",
      "5                                       North Indian   \n",
      "6                              North Indian, Mughlai   \n",
      "7   North Indian, Continental, Chinese, South Indian   \n",
      "\n",
      "                                            Location Ratings  \\\n",
      "0                     Connaught Place, Central Delhi       4   \n",
      "1  The Leela Ambience Convention Hotel,Shahdara, ...     4.3   \n",
      "2               Hilton Garden Inn,Saket, South Delhi     3.9   \n",
      "3                 Gardens Galleria,Sector 38A, Noida     3.9   \n",
      "4     Taurus Sarovar Portico,Mahipalpur, South Delhi     3.7   \n",
      "5  Indirapuram Habitat Centre,Indirapuram, Ghaziabad     3.8   \n",
      "6   Suncity Business Tower,Golf Course Road, Gurgaon     4.3   \n",
      "7              M2K Corporate Park,Sector 51, Gurgaon     4.1   \n",
      "\n",
      "                                           Image url  \n",
      "0  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "1  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "2  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "3  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "4  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "5  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "6  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "7  https://im1.dineout.co.in/images/uploads/resta...  \n"
     ]
    }
   ],
   "source": [
    "#importing the required libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def dine(url):\n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    titles=[] #empty list for storing yhe titles\n",
    "    for i in soup.find_all('a', class_=\"restnt-name ellipsis\"):\n",
    "        titles.append(i.text)\n",
    "    \n",
    "    locations=[] #empty list\n",
    "    for i in soup.find_all('div', class_=\"restnt-loc ellipsis\"):\n",
    "        locations.append(i.text)\n",
    "    \n",
    "    cuisine=[]\n",
    "    for i in soup.find_all('div',class_=\"detail-info\"):\n",
    "        cuisine.append(i.text.split('|')[1])\n",
    "        \n",
    "    images=[]\n",
    "    for i in soup.find_all(\"img\",class_=\"no-img\"):\n",
    "        images.append(i.get('data-src'))\n",
    "        \n",
    "    ratings=[]\n",
    "    for i in soup.find_all(\"div\",class_=\"restnt-rating rating-4\"):\n",
    "        ratings.append(i.text)\n",
    "\n",
    "    df=pd.DataFrame({'Restaurant Name':titles,'Cuisine':cuisine,'Location':locations,'Ratings':ratings,'Image url':images})\n",
    "    print(df)\n",
    "    \n",
    "\n",
    "dine('https://www.dineout.co.in/delhi-restaurants/buffet-special')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cbf350",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
